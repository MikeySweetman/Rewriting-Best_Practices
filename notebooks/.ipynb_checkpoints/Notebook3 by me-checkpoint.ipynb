{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e33d28",
   "metadata": {},
   "source": [
    "# Data Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88dcfb",
   "metadata": {},
   "source": [
    "Here, we will show some simple examples of featurizing materials composition data using so-called \"composition-based feature vectors\", or CBFVs. This methods represents a single chemical formula as one vector based on its constituent atoms' chemical properties (refer to the paper for more information and references).\n",
    "\n",
    "Note that the steps shown in this notebook are intended to demonstrate the best practices associated with featurizing materials data, using *one* way of featurizing materials composition data as an example. \n",
    "Depending on your input data and your particular modeling needs, the data featurization method and procedure you use may be different than the example shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf705ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e547a",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "\n",
    "We will start with the dataset splits that we saved from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5d34ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train dataframe shape is (3214, 3)\n",
      "df_val dataframe shape is (980, 3)\n",
      "df_test dataframe shape is (370, 3)\n"
     ]
    }
   ],
   "source": [
    "#getting the paths for all the data\n",
    "PATH = os.getcwd()\n",
    "train_path = os.path.join(PATH, \"../data_for_notebook_bestpractice/cp_train_byme.csv\")\n",
    "val_path = os.path.join(PATH, \"../data_for_notebook_bestpractice/cp_val_byme.csv\")\n",
    "test_path = os.path.join(PATH, \"../data_for_notebook_bestpractice/cp_test_byme.csv\")\n",
    "\n",
    "#now that the paths are created, we can make the dataframes\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"df_train dataframe shape is\", df_train.shape)\n",
    "print(\"df_val dataframe shape is\", df_val.shape)\n",
    "print(\"df_test dataframe shape is\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758dd43",
   "metadata": {},
   "source": [
    "## Sub-sampling your data (optional)\n",
    "\n",
    "If your dataset is too large, you can subsample it to be a smaller size.\n",
    "This is useful for prototyping and for making quick sanity tests of new models / parameters.\n",
    "\n",
    "Just be aware that you do not introduce any bias into your data through the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7c1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_train-sample:(2000, 3)\n",
      "shape of df_val_sample:(200, 3)\n",
      "shape of df_test_sample(200, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.sample(n = 2000, random_state = RNG_SEED) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "df_val_sample = df_val.sample(n = 200, random_state = RNG_SEED)\n",
    "df_test_sample = df_test.sample(n = 200, random_state = RNG_SEED)\n",
    "\n",
    "print(f\"shape of df_train-sample:{df_train_sample.shape}\\nshape of df_val_sample:{df_val_sample.shape}\\nshape of df_test_sample{df_test_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8c122",
   "metadata": {},
   "source": [
    "## Generate features using the `CBFV` package\n",
    "\n",
    "To featurize the chemical compositions from a chemical formula (e.g. \"Al2O3\") into a composition-based feature vector (CBFV), we use the open-source [`CBFV` package](https://github.com/kaaiian/CBFV).\n",
    "\n",
    "We have downloaded and saved a local copy of the package into this repository for your convenience.\n",
    "For the most updated version, refer to the GitHub repository linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9ae852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the package and the generate_features function\n",
    "from CBFV.cbfv.composition import generate_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278ed3b",
   "metadata": {},
   "source": [
    "The `generate_features` function from the CBFV package expects an input DataFrame containing at least the columns `['formula', 'target']`. You may also have extra feature columns (e.g., `temperature` or `pressure`, other measurement conditions, etc.).\n",
    "\n",
    "In our dataset, `Cp` represents the target variable, and `T` is the measurement condition.\n",
    "Since the `generate_features` function expects the target variable column to be named `target`, we have to rename the `Cp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff4f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame column names before renaming:\n",
      "Index(['formula', 'T', 'Cp'], dtype='object')\n",
      "Index(['formula', 'T', 'Cp'], dtype='object')\n",
      "Index(['formula', 'T', 'Cp'], dtype='object')\n",
      "\n",
      "DataFrame column names after renaming:\n",
      "Index(['formula', 'T', 'target'], dtype='object')\n",
      "Index(['formula', 'T', 'target'], dtype='object')\n",
      "Index(['formula', 'T', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('DataFrame column names before renaming:')\n",
    "print(df_train.columns)\n",
    "print(df_val.columns)\n",
    "print(df_test.columns)\n",
    "\n",
    "#renaming Cp column to \"target\"\n",
    "rename_cp = {\"Cp\" : \"target\"}\n",
    "df_train = df_train.rename(columns = rename_cp)\n",
    "df_val = df_val.rename(columns = rename_cp)\n",
    "df_test = df_test.rename(columns = rename_cp)\n",
    "\n",
    "df_train_sample = df_train_sample.rename(columns = rename_cp)\n",
    "df_val_sample = df_val_sample.rename(columns = rename_cp)\n",
    "df_test_sample = df_test_sample.rename(columns = rename_cp)\n",
    "\n",
    "print('\\nDataFrame column names after renaming:')\n",
    "print(df_train.columns)\n",
    "print(df_val.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1361ef3",
   "metadata": {},
   "source": [
    "Now we can use the `generate_features` function to generate the CBFVs from the input data.\n",
    "\n",
    "Note that we have specified several keyword arguments in our call to `generate_features`:\n",
    "* `elem_prop='oliynyk'`\n",
    "* `drop_duplicates=False`\n",
    "* `extend_features=True`\n",
    "* `sum_feat=True`\n",
    "\n",
    "A short explanation for the choice of keyword arguments is below:\n",
    "* The `elem_prop` parameter specifies which CBFV featurization scheme to use (there are several). For this tutorial, we have chosen to use the `oliynyk` CBFV featurization scheme.\n",
    "* The `drop_duplicates` parameter specifies whether to drop duplicate formulae during featurization. In our case, we want to preserve duplicate formulae in our data (`True`), since we have multiple heat capacity measurements (performed at different temperatures) for the same compound.\n",
    "* The `extend_features` parameter specifies whether to include extended features (features that are not part of `['formula', 'target']`) in the featurized data. In our case, this is our measurement temperature, and we want to include this information (`True`), since this is pertinent information for the heat capacity prediction.\n",
    "* The `sum_feat` parameter specifies whether to calculate the sum features when generating the CBFVs for the chemical formulae. We do in our case (`True`).\n",
    "\n",
    "For more information about the `generate_features` function and the CBFV featurization scheme, refer to the GitHub repository and the accompanying paper to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9971e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data: 100%|████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 14302.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeaturizing Compositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Features...: 100%|█████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 8416.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating Pandas Objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data: 100%|██████████████████████████████████████████████████████| 200/200 [00:00<00:00, 14359.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeaturizing Compositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Features...: 100%|███████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4332.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating Pandas Objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data: 100%|██████████████████████████████████████████████████████| 200/200 [00:00<00:00, 17487.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeaturizing Compositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Features...: 100%|███████████████████████████████████████████████████████| 200/200 [00:00<00:00, 5505.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating Pandas Objects...\n"
     ]
    }
   ],
   "source": [
    "#note that what is labelled here is unscaled!!!\n",
    "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train_sample, elem_prop = 'oliynyk', drop_duplicates = False, extend_features = True, sum_feat = True)\n",
    "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val_sample, elem_prop = 'oliynyk', drop_duplicates = False, extend_features = True, sum_feat = True)\n",
    "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test_sample, elem_prop = 'oliynyk', drop_duplicates = False, extend_features = True, sum_feat = True)\n",
    "\n",
    "#wtf is this\n",
    "#need to figure out what the hell this is\n",
    "\n",
    "#I think drop_duplicates will remove a formula if seen more than once i.e. only keep first example of it\n",
    "#for Cp there will be same formula at many different temps so we don't want to get rid of duplicates\n",
    "#therefore it is set to false\n",
    "\n",
    "#similar to splittig data into train/val/test am unsure how to know what to put to the left of the = sign\n",
    "#i.e. why is it X_train_unscaled, y_train, formulae_train, skipped_train and in that order\n",
    "\n",
    "#think that sum means if H2o2 atomic no = 34 etc. adds up values associated with each element in compound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ed19e",
   "metadata": {},
   "source": [
    "To see what a featurized X matrix looks like, `.head()` will show us some rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813f7ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_Atomic_Number</th>\n",
       "      <th>sum_Atomic_Weight</th>\n",
       "      <th>sum_Period</th>\n",
       "      <th>sum_group</th>\n",
       "      <th>sum_families</th>\n",
       "      <th>sum_Metal</th>\n",
       "      <th>sum_Nonmetal</th>\n",
       "      <th>sum_Metalliod</th>\n",
       "      <th>sum_Mendeleev_Number</th>\n",
       "      <th>sum_l_quantum_number</th>\n",
       "      <th>...</th>\n",
       "      <th>range_Melting_point_(K)</th>\n",
       "      <th>range_Boiling_Point_(K)</th>\n",
       "      <th>range_Density_(g/mL)</th>\n",
       "      <th>range_specific_heat_(J/g_K)_</th>\n",
       "      <th>range_heat_of_fusion_(kJ/mol)_</th>\n",
       "      <th>range_heat_of_vaporization_(kJ/mol)_</th>\n",
       "      <th>range_thermal_conductivity_(W/(m_K))_</th>\n",
       "      <th>range_heat_atomization(kJ/mol)</th>\n",
       "      <th>range_Cohesive_energy</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>65.116040</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.642621e+06</td>\n",
       "      <td>4.742507e+06</td>\n",
       "      <td>0.858492</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>2388.183171</td>\n",
       "      <td>22965.815879</td>\n",
       "      <td>3091.366423</td>\n",
       "      <td>66594.888889</td>\n",
       "      <td>7.034755</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>53.491200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.363940e+03</td>\n",
       "      <td>8.544527e+03</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>40.816697</td>\n",
       "      <td>1.696930</td>\n",
       "      <td>17.270367</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>22037.555556</td>\n",
       "      <td>4.284089</td>\n",
       "      <td>457.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>98.887792</td>\n",
       "      <td>14.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.009057e+05</td>\n",
       "      <td>1.662798e+06</td>\n",
       "      <td>0.601941</td>\n",
       "      <td>1.321867</td>\n",
       "      <td>10.138486</td>\n",
       "      <td>13933.526946</td>\n",
       "      <td>6716.921700</td>\n",
       "      <td>10368.666667</td>\n",
       "      <td>1.070067</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>41.988171</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.521744e+04</td>\n",
       "      <td>2.868138e+05</td>\n",
       "      <td>0.234886</td>\n",
       "      <td>0.042025</td>\n",
       "      <td>1.372178</td>\n",
       "      <td>2194.463394</td>\n",
       "      <td>4968.283245</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>2800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.0</td>\n",
       "      <td>207.200000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sum_Atomic_Number  sum_Atomic_Weight  sum_Period  sum_group  sum_families  \\\n",
       "0               32.0          65.116040         8.0       30.0          15.0   \n",
       "1               28.0          53.491200         9.0       36.0          43.0   \n",
       "2               46.0          98.887792        14.0       72.0          36.0   \n",
       "3               20.0          41.988171         5.0       18.0           9.0   \n",
       "4               82.0         207.200000         6.0       14.0           5.0   \n",
       "\n",
       "   sum_Metal  sum_Nonmetal  sum_Metalliod  sum_Mendeleev_Number  \\\n",
       "0        1.0           2.0            0.0                 162.0   \n",
       "1        0.0           6.0            0.0                 544.0   \n",
       "2        3.0           4.0            0.0                 441.0   \n",
       "3        1.0           1.0            0.0                  95.0   \n",
       "4        1.0           0.0            0.0                  81.0   \n",
       "\n",
       "   sum_l_quantum_number  ...  range_Melting_point_(K)  \\\n",
       "0                   2.0  ...             2.642621e+06   \n",
       "1                   2.0  ...             4.363940e+03   \n",
       "2                   4.0  ...             4.009057e+05   \n",
       "3                   1.0  ...             2.521744e+04   \n",
       "4                   1.0  ...             0.000000e+00   \n",
       "\n",
       "   range_Boiling_Point_(K)  range_Density_(g/mL)  \\\n",
       "0             4.742507e+06              0.858492   \n",
       "1             8.544527e+03              0.000002   \n",
       "2             1.662798e+06              0.601941   \n",
       "3             2.868138e+05              0.234886   \n",
       "4             0.000000e+00              0.000000   \n",
       "\n",
       "   range_specific_heat_(J/g_K)_  range_heat_of_fusion_(kJ/mol)_  \\\n",
       "0                      0.021622                     2388.183171   \n",
       "1                     40.816697                        1.696930   \n",
       "2                      1.321867                       10.138486   \n",
       "3                      0.042025                        1.372178   \n",
       "4                      0.000000                        0.000000   \n",
       "\n",
       "   range_heat_of_vaporization_(kJ/mol)_  \\\n",
       "0                          22965.815879   \n",
       "1                             17.270367   \n",
       "2                          13933.526946   \n",
       "3                           2194.463394   \n",
       "4                              0.000000   \n",
       "\n",
       "   range_thermal_conductivity_(W/(m_K))_  range_heat_atomization(kJ/mol)  \\\n",
       "0                            3091.366423                    66594.888889   \n",
       "1                               0.006030                    22037.555556   \n",
       "2                            6716.921700                    10368.666667   \n",
       "3                            4968.283245                      225.000000   \n",
       "4                               0.000000                        0.000000   \n",
       "\n",
       "   range_Cohesive_energy       T  \n",
       "0               7.034755   600.0  \n",
       "1               4.284089   457.7  \n",
       "2               1.070067   300.0  \n",
       "3               0.018632  2800.0  \n",
       "4               0.000000  1400.0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155a70c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 177)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unscaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da18ba4",
   "metadata": {},
   "source": [
    "Note the `sum` features in the CBFV, which we have included by using `sum_feat=True` in the call to `generate_features`.\n",
    "\n",
    "Also note the temperature column `T` at the end of this featurized data.\n",
    "\n",
    "What we have done above is featurize the input data. In the featurized data, each row contains a unique CBFV that describes a given chemical composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b77ce",
   "metadata": {},
   "source": [
    "## Data scaling & normalization\n",
    "\n",
    "For numerical input data, scaling and normalization of the features often improves the model performance.\n",
    "Scaling can partially correct the discrepancy between the orders of magnitudes of the features (e.g., some numerical features being much larger or smaller than others).\n",
    "This typically improves the model learning performance, and in turn, improves the model performance.\n",
    "\n",
    "We will scale then normalize our input data using scikit-learn's built-in `StandardScaler` class and `normalize` function.\n",
    "\n",
    "Note, in addition to `StandardScaler`, other scalers such as `RobustScaler` and `MinMaxScaler` are also available in scikit-learn. Consult the documentation for the details and when to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acd0f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960d55a",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "First, we instantiate the scaler object.\n",
    "\n",
    "In a `StandardScaler` object:\n",
    "* During the `fit` process, the statistics of the input data (mean and standard deviation) are computed.\n",
    "* Then, during the `transform` process, the mean and standard deviation values calculated above are used to scale the data to having zero-mean and unit variance.\n",
    "\n",
    "Therefore, for the first time usage of the scaler, we call the `.fit_transform()` method to fit the scaler to the input data, and then to transform the same data.\n",
    "For subsequent uses, since we have already computed the statistics, we only call the `.transform()` method to scale data.\n",
    "\n",
    "# **Note:** you should *only* `.fit()` the scaler using the training dataset statistics, and then use these same statistics from the training dataset to `.transform()` the other datasets (validation and train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81288b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always scale first and then normalise!!!\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_unscaled)\n",
    "X_val = scaler.transform(X_val_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41323e5c",
   "metadata": {},
   "source": [
    "## Normalizing the scaled data\n",
    "\n",
    "We repeat a similar process for normalizing the data.\n",
    "Here, there is no need to first fit the normalizer, since the normalizer scales the rows of the input data to unit norm independently of other rows.\n",
    "\n",
    "The normalizer is different to a Scaler in that the normalizer acts row-wise, whereas a Scaler acts column-wise on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9544b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13759c1f",
   "metadata": {},
   "source": [
    "# Modeling using \"classical\" machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51982192",
   "metadata": {},
   "source": [
    "Here we implement some classical ML models from `sklearn`:\n",
    "\n",
    "* Ridge regression\n",
    "* Support vector machine\n",
    "* Linear support vector machine\n",
    "* Random forest\n",
    "* Extra trees\n",
    "* Adaptive boosting\n",
    "* Gradient boosting\n",
    "* k-nearest neighbors\n",
    "* Dummy (if you can't beat this, something is wrong.)\n",
    "\n",
    "Note: the Dummy model types from `sklearn` act as a good sanity check for your ML studies. If your models do not perform significantly better than the equivalent Dummy models, then you should know that something has gone wrong in your model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bee7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57225751",
   "metadata": {},
   "source": [
    "In addition, we define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fb7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#should probably figure out what the hell this stuff is\n",
    "#no clue what a lot of the stuff here is I need to dig into it definitely\n",
    "\n",
    "#tells you which model is being usitilised?\n",
    "def instantiate_model(model_name):\n",
    "    model = model_name() #going from model_name to model_name() turns it into an object I think?\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X_train, y_train):\n",
    "    ti = time()\n",
    "    model = instantiate_model(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time() - ti\n",
    "    return model, fit_time\n",
    "\n",
    "def evaluate_model(model, X, y_act):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_act, y_pred)\n",
    "    mae = mean_absolute_error(y_act, y_pred)\n",
    "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
    "    return r2, mae, rmse_val\n",
    "\n",
    "def fit_evaluate_model(model, model_name, X_train, y_train, X_val, y_act_val):\n",
    "    model, fit_time = fit_model(model, X_train, y_train)\n",
    "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
    "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_act_val)\n",
    "    result_dict = {\n",
    "        'model_name': model_name,\n",
    "        'model_name_pretty': type(model).__name__,\n",
    "        'model_params': model.get_params(),\n",
    "        'fit_time': fit_time,\n",
    "        'r2_train': r2_train,\n",
    "        'mae_train': mae_train,\n",
    "        'rmse_train': rmse_train,\n",
    "        'r2_val': r2_val,\n",
    "        'mae_val': mae_val,\n",
    "        'rmse_val': rmse_val}\n",
    "    return model, result_dict\n",
    "\n",
    "def append_result_df(df, result_dict):\n",
    "    df_result_appended = df.append(result_dict, ignore_index=True)\n",
    "    return df_result_appended\n",
    "\n",
    "def append_model_dict(dic, model_name, model):\n",
    "    dic[model_name] = model\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851debc5",
   "metadata": {},
   "source": [
    "Build an empty DataFrame to store model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dde44fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_name_pretty</th>\n",
       "      <th>model_params</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>mae_val</th>\n",
       "      <th>rmse_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, model_name_pretty, model_params, fit_time, r2_train, mae_train, rmse_train, r2_val, mae_val, rmse_val]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classics = pd.DataFrame(columns=['model_name',\n",
    "                                    'model_name_pretty', #wtd is model name pretty like\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "df_classics\n",
    "#no clue about this stuff need to look into it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d56c8b",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Here, we instantiate several classical machine learning models for use.\n",
    "For demonstration purposes, we instantiate the models with their default model parameters.\n",
    "\n",
    "Some of the models listed above can perform either regression or classification tasks.\n",
    "Because our ML task is a regression task (prediction of the continuous-valued target, heat capacity), we choose the regression variant of these models.\n",
    "\n",
    "Note: the `DummyRegressor()` instance acts as a good sanity check for your ML studies. If your models do not perform significantly better than the `DummyRegressor()`, then you know something has gone awry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9df583ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of model names\n",
    "classic_model_names = OrderedDict({ #need to find out what OrderedDict is\n",
    "    'dumr': DummyRegressor,\n",
    "    'rr': Ridge,\n",
    "    'abr': AdaBoostRegressor,\n",
    "    'gbr': GradientBoostingRegressor,\n",
    "    'rfr': RandomForestRegressor,\n",
    "    'etr': ExtraTreesRegressor,\n",
    "    'svr': SVR,\n",
    "    'lsvr': LinearSVR,\n",
    "    'knr': KNeighborsRegressor,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6f0c6",
   "metadata": {},
   "source": [
    "## Instantiate and fit the models\n",
    "\n",
    "Now, we can fit the ML models.\n",
    "\n",
    "We will loop through each of the models listed above. For each of the models, we will:\n",
    "* instantiate the model (with default parameters)\n",
    "* fit the model using the training data\n",
    "* use the fitted model to generate predictions from the validation data\n",
    "* evaluate the performance of the model using the predictions\n",
    "* store the results in a DataFrame for analysis\n",
    "\n",
    "Note: this may take several minutes, depending on your hardware/software environment, dataset size and featurization scheme (CBFV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316cb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
